{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZN4imr7vrr7Fh/BjKQjsf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivekvj18/ML_PROJECT/blob/main/ML_Project(91_019).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-8XJLP0wFBW",
        "outputId": "795b4646-c14b-4082-ab7f-e58e8706ab5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data Loaded Successfully!\n",
            "Shape: (15533, 18)\n",
            "Columns: ['id', 'Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight', 'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS', 'WeightCategory']\n",
            "Train shape: (12426, 18), Test shape: (3107, 18)\n",
            "\n",
            "üöÄ Training XGBoost Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [11:07:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Training Complete!\n",
            "\n",
            "üìä Model Performance:\n",
            "Accuracy: 0.9051\n",
            "\n",
            "Classification Report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "Insufficient_Weight       0.92      0.94      0.93       374\n",
            "      Normal_Weight       0.89      0.89      0.89       469\n",
            "     Obesity_Type_I       0.89      0.86      0.88       441\n",
            "    Obesity_Type_II       0.96      0.97      0.96       481\n",
            "   Obesity_Type_III       0.99      1.00      0.99       597\n",
            " Overweight_Level_I       0.81      0.75      0.78       369\n",
            "Overweight_Level_II       0.81      0.85      0.83       376\n",
            "\n",
            "           accuracy                           0.91      3107\n",
            "          macro avg       0.90      0.90      0.90      3107\n",
            "       weighted avg       0.90      0.91      0.90      3107\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[353  19   0   0   0   2   0]\n",
            " [ 26 417   0   0   0  22   4]\n",
            " [  0   0 380  18   4  12  27]\n",
            " [  0   0   9 468   2   0   2]\n",
            " [  0   0   0   0 596   1   0]\n",
            " [  4  31  13   0   0 277  44]\n",
            " [  0   1  23   3   0  28 321]]\n"
          ]
        }
      ],
      "source": [
        "# ==================================================\n",
        "# CLEAN XGBOOST TRAINING (80-20 SPLIT)\n",
        "# From scratch: Data Cleaning + Preprocessing + Training + Accuracy\n",
        "# ==================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# 1Ô∏è‚É£ Load Data\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "print(\"‚úÖ Data Loaded Successfully!\")\n",
        "print(\"Shape:\", train_df.shape)\n",
        "print(\"Columns:\", train_df.columns.tolist())\n",
        "\n",
        "# 2Ô∏è‚É£ Basic Cleaning\n",
        "# Drop duplicates\n",
        "train_df = train_df.drop_duplicates()\n",
        "\n",
        "# Handle missing values\n",
        "train_df = train_df.fillna(train_df.mode().iloc[0])  # Fill all missing values with mode\n",
        "\n",
        "# 3Ô∏è‚É£ Feature Engineering\n",
        "train_df['BMI'] = train_df['Weight'] / (train_df['Height']**2)\n",
        "train_df['AgeGroup'] = pd.cut(\n",
        "    train_df['Age'],\n",
        "    bins=[0, 18, 30, 45, 60, 100],\n",
        "    labels=['Teen', 'Young', 'Adult', 'MidAge', 'Senior']\n",
        ")\n",
        "\n",
        "# 4Ô∏è‚É£ Separate features and target\n",
        "X = train_df.drop(columns=['WeightCategory', 'id'], errors='ignore')\n",
        "y = train_df['WeightCategory']\n",
        "\n",
        "# Encode target\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# 5Ô∏è‚É£ Split data into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
        ")\n",
        "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
        "\n",
        "# 6Ô∏è‚É£ Identify categorical and numeric columns\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
        "numeric_features = X.select_dtypes(include=np.number).columns\n",
        "\n",
        "# 7Ô∏è‚É£ Preprocessing\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 8Ô∏è‚É£ Model Definition (Basic XGBoost)\n",
        "xgb_model = XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    num_class=len(le.classes_),\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=300,\n",
        "    max_depth=6,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    gamma=0.1,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "\n",
        "# 9Ô∏è‚É£ Pipeline\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', xgb_model)\n",
        "])\n",
        "\n",
        "# üîü Train the model\n",
        "print(\"\\nüöÄ Training XGBoost Model...\")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"‚úÖ Training Complete!\")\n",
        "\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"\\nüìä Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# ‚úÖ Predict on test (20%) data\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# ‚úÖ Calculate accuracy\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"\\nüéØ Accuracy on 20% Test Data: {test_accuracy:.4f}\")\n",
        "\n",
        "# ‚úÖ Detailed metrics\n",
        "print(\"\\nClassification Report on 20% Test Data:\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix on 20% Test Data:\")\n",
        "print(confusion_matrix(y_test, y_test_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ7JTYCxwqeo",
        "outputId": "33ea807b-4c18-49c8-fab3-6b2c57398ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ Accuracy on 20% Test Data: 0.9051\n",
            "\n",
            "Classification Report on 20% Test Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93       374\n",
            "           1       0.89      0.89      0.89       469\n",
            "           2       0.89      0.86      0.88       441\n",
            "           3       0.96      0.97      0.96       481\n",
            "           4       0.99      1.00      0.99       597\n",
            "           5       0.81      0.75      0.78       369\n",
            "           6       0.81      0.85      0.83       376\n",
            "\n",
            "    accuracy                           0.91      3107\n",
            "   macro avg       0.90      0.90      0.90      3107\n",
            "weighted avg       0.90      0.91      0.90      3107\n",
            "\n",
            "\n",
            "Confusion Matrix on 20% Test Data:\n",
            "[[353  19   0   0   0   2   0]\n",
            " [ 26 417   0   0   0  22   4]\n",
            " [  0   0 380  18   4  12  27]\n",
            " [  0   0   9 468   2   0   2]\n",
            " [  0   0   0   0 596   1   0]\n",
            " [  4  31  13   0   0 277  44]\n",
            " [  0   1  23   3   0  28 321]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "# Identify features\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "numeric_features = X_train.select_dtypes(include=np.number).columns\n",
        "\n",
        "# Preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Base model\n",
        "num_classes = len(np.unique(y_train))\n",
        "xgb_base = XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    num_class=num_classes,\n",
        "    eval_metric='mlogloss',\n",
        "    use_label_encoder=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', xgb_base)\n",
        "])\n",
        "\n",
        "# Hyperparameter distributions\n",
        "param_dist = {\n",
        "    'classifier__max_depth': randint(4, 9),\n",
        "    'classifier__learning_rate': uniform(0.01, 0.09),\n",
        "    'classifier__n_estimators': randint(300, 900),\n",
        "    'classifier__subsample': uniform(0.7, 0.3),\n",
        "    'classifier__colsample_bytree': uniform(0.7, 0.3),\n",
        "    'classifier__min_child_weight': randint(1, 6),\n",
        "    'classifier__gamma': uniform(0, 0.3)\n",
        "}\n",
        "\n",
        "# Randomized Search\n",
        "random_search = RandomizedSearchCV(\n",
        "    pipeline,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=25,  # smaller for speed\n",
        "    scoring='accuracy',\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best estimator\n",
        "best_model = random_search.best_estimator_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSvWZJeew2vL",
        "outputId": "ed36f324-066e-4c82-f955-961cbd7d1406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [11:15:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# XGBoost Full Pipeline with Faster Hypertuning\n",
        "# ===============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -------------------------------\n",
        "# 1Ô∏è‚É£ Load Data\n",
        "# -------------------------------\n",
        "train_df = pd.read_csv(\"train.csv\")  # make sure train.csv is uploaded\n",
        "\n",
        "# -------------------------------\n",
        "# 2Ô∏è‚É£ Feature Engineering\n",
        "# -------------------------------\n",
        "train_df['BMI'] = train_df['Weight'] / (train_df['Height']**2)\n",
        "train_df['AgeGroup'] = pd.cut(\n",
        "    train_df['Age'],\n",
        "    bins=[0, 18, 30, 45, 60, 100],\n",
        "    labels=['Teen', 'Young', 'Adult', 'MidAge', 'Senior']\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 3Ô∏è‚É£ Separate features and target\n",
        "# -------------------------------\n",
        "X = train_df.drop(columns=['id', 'WeightCategory'], errors='ignore')\n",
        "y = train_df['WeightCategory']\n",
        "\n",
        "# -------------------------------\n",
        "# 4Ô∏è‚É£ Train-Test Split (80%-20%)\n",
        "# -------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 5Ô∏è‚É£ Preprocessing Pipeline\n",
        "# -------------------------------\n",
        "# Identify categorical and numeric features\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
        "numeric_features = X.select_dtypes(include=np.number).columns\n",
        "\n",
        "# Transformers\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Encode target\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_test_enc = le.transform(y_test)\n",
        "\n",
        "# -------------------------------\n",
        "# 6Ô∏è‚É£ XGBoost Model & RandomizedSearchCV\n",
        "# -------------------------------\n",
        "xgb_model = XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    eval_metric='mlogloss',\n",
        "    use_label_encoder=False,\n",
        "    n_jobs=-1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', xgb_model)\n",
        "])\n",
        "\n",
        "# Faster hypertuning\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': [200, 400, 600],\n",
        "    'classifier__max_depth': [4, 5, 6],\n",
        "    'classifier__learning_rate': [0.01, 0.03, 0.05],\n",
        "    'classifier__subsample': [0.8, 0.85],\n",
        "    'classifier__colsample_bytree': [0.8, 0.85],\n",
        "    'classifier__gamma': [0, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    pipeline,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=15,  # random 15 combinations\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 7Ô∏è‚É£ Fit RandomizedSearchCV\n",
        "# -------------------------------\n",
        "print(\"üöÄ Starting Faster XGBoost Hypertuning...\")\n",
        "random_search.fit(X_train, y_train_enc)\n",
        "print(\"‚úÖ Hypertuning Complete!\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8Ô∏è‚É£ Evaluate on Train & Test\n",
        "# -------------------------------\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "print(f\"Best Hyperparameters: {random_search.best_params_}\\n\")\n",
        "print(f\"Train Accuracy: {accuracy_score(y_train_enc, y_train_pred):.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test_enc, y_test_pred):.4f}\\n\")\n",
        "\n",
        "print(\"Classification Report (Test Set):\")\n",
        "print(classification_report(y_test_enc, y_test_pred, target_names=le.classes_))\n",
        "\n",
        "print(\"Confusion Matrix (Test Set):\")\n",
        "print(confusion_matrix(y_test_enc, y_test_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlOXj5JTzAVv",
        "outputId": "12767731-3a2e-4cd2-f4f5-03da8651318a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Faster XGBoost Hypertuning...\n",
            "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
            "‚úÖ Hypertuning Complete!\n",
            "Best Hyperparameters: {'classifier__subsample': 0.8, 'classifier__n_estimators': 400, 'classifier__max_depth': 4, 'classifier__learning_rate': 0.05, 'classifier__gamma': 0.2, 'classifier__colsample_bytree': 0.85}\n",
            "\n",
            "Train Accuracy: 0.9485\n",
            "Test Accuracy: 0.9063\n",
            "\n",
            "Classification Report (Test Set):\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "Insufficient_Weight       0.93      0.95      0.94       374\n",
            "      Normal_Weight       0.89      0.91      0.90       469\n",
            "     Obesity_Type_I       0.89      0.87      0.88       441\n",
            "    Obesity_Type_II       0.96      0.97      0.97       481\n",
            "   Obesity_Type_III       0.99      1.00      0.99       597\n",
            " Overweight_Level_I       0.82      0.75      0.78       369\n",
            "Overweight_Level_II       0.80      0.84      0.82       376\n",
            "\n",
            "           accuracy                           0.91      3107\n",
            "          macro avg       0.90      0.90      0.90      3107\n",
            "       weighted avg       0.91      0.91      0.91      3107\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[354  18   0   0   0   2   0]\n",
            " [ 22 425   0   0   0  18   4]\n",
            " [  0   0 382  16   4  12  27]\n",
            " [  0   0  10 468   1   0   2]\n",
            " [  0   0   1   0 595   1   0]\n",
            " [  4  32  13   0   0 275  45]\n",
            " [  0   2  24   4   0  29 317]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# FINAL SUBMISSION CODE WITH TUNED XGBOOST\n",
        "# ==================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from google.colab import files\n",
        "\n",
        "# --- Load Data ---\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df_original = pd.read_csv(\"test.csv\")  # Keep original 'id'\n",
        "\n",
        "# --- Feature Engineering ---\n",
        "for df in [train_df, test_df_original]:\n",
        "    df['BMI'] = df['Weight'] / (df['Height']**2)\n",
        "    df['AgeGroup'] = pd.cut(\n",
        "        df['Age'],\n",
        "        bins=[0, 18, 30, 45, 60, 100],\n",
        "        labels=['Teen', 'Young', 'Adult', 'MidAge', 'Senior']\n",
        "    )\n",
        "\n",
        "# --- Prepare Training Data ---\n",
        "X = train_df.drop(columns=['WeightCategory', 'id'], errors='ignore')\n",
        "y = train_df['WeightCategory']\n",
        "\n",
        "# Fill missing AgeGroup if any\n",
        "if 'AgeGroup' in X.columns and X['AgeGroup'].isnull().sum() > 0:\n",
        "    mode_age_group = X['AgeGroup'].mode()[0]\n",
        "    X['AgeGroup'].fillna(mode_age_group, inplace=True)\n",
        "else:\n",
        "    mode_age_group = X['AgeGroup'].mode()[0] if 'AgeGroup' in X.columns else None\n",
        "\n",
        "# Identify numeric and categorical features\n",
        "numeric_features = X.select_dtypes(include=np.number).columns\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Preprocessing pipeline\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Encode target\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# --- Define Final XGBoost Model with Tuned Hyperparameters ---\n",
        "final_xgb = XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    num_class=len(le.classes_),\n",
        "    n_estimators=400,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=5,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    gamma=0.2,\n",
        "    eval_metric='mlogloss',\n",
        "    use_label_encoder=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# --- Full Pipeline ---\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', final_xgb)\n",
        "])\n",
        "\n",
        "# --- Train on FULL training data ---\n",
        "print(\"üöÄ Training final XGBoost model on full training data...\")\n",
        "pipeline.fit(X, y_encoded)\n",
        "print(\"‚úÖ Training complete!\")\n",
        "\n",
        "# --- Prepare Test Data ---\n",
        "X_test = test_df_original.drop(columns=['id', 'WeightCategory'], errors='ignore')\n",
        "if 'AgeGroup' in X_test.columns and X_test['AgeGroup'].isnull().sum() > 0 and mode_age_group is not None:\n",
        "    X_test['AgeGroup'].fillna(mode_age_group, inplace=True)\n",
        "\n",
        "# --- Predict ---\n",
        "test_pred_encoded = pipeline.predict(X_test)\n",
        "test_pred_labels = le.inverse_transform(test_pred_encoded)\n",
        "print(\"‚úÖ Test predictions generated!\")\n",
        "\n",
        "# --- Create Submission ---\n",
        "submission_file = 'kaggle_submission_xgb_final_withhypertuning.csv'\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': test_df_original['id'],\n",
        "    'WeightCategory': test_pred_labels\n",
        "})\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "print(f\"‚úÖ Submission file created: {submission_file}\")\n",
        "\n",
        "# Uncomment to download automatically in Colab\n",
        "# files.download(submission_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LcsrZbA0nmB",
        "outputId": "46ee872b-c27d-468b-f930-59c89ac7431d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Training final XGBoost model on full training data...\n",
            "‚úÖ Training complete!\n",
            "‚úÖ Test predictions generated!\n",
            "‚úÖ Submission file created: kaggle_submission_xgb_final_withhypertuning.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# XGBoost Hypertuning + Final Model + Submission\n",
        "# =============================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Load Data\n",
        "# -----------------------------\n",
        "print(\"‚úÖ Data Loaded Successfully!\")\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "print(\"Train shape:\", train_df.shape, \", Test shape:\", test_df.shape)\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Feature Engineering (Apply to both train and test)\n",
        "# -----------------------------\n",
        "for df in [train_df, test_df]:\n",
        "    df['BMI'] = df['Weight'] / (df['Height']**2)\n",
        "    df['AgeGroup'] = pd.cut(\n",
        "        df['Age'],\n",
        "        bins=[0, 18, 30, 45, 60, 100],\n",
        "        labels=['Teen', 'Young', 'Adult', 'MidAge', 'Senior']\n",
        "    )\n",
        "    # Fill missing AgeGroup if any\n",
        "    if 'AgeGroup' in df.columns and df['AgeGroup'].isnull().sum() > 0:\n",
        "        mode_age_group = df['AgeGroup'].mode()[0]\n",
        "        df['AgeGroup'].fillna(mode_age_group, inplace=True)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Separate features and target\n",
        "# -----------------------------\n",
        "X = train_df.drop(columns=[\"WeightCategory\", 'id'])   # Dropping 'id' as well\n",
        "y = train_df[\"WeightCategory\"]\n",
        "\n",
        "# Encode categorical target if necessary\n",
        "if y.dtype == 'object':\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y)\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Split Train (80%) / Validation (20%)\n",
        "# -----------------------------\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Preprocessing Pipeline\n",
        "# -----------------------------\n",
        "# Identify categorical and numeric features (based on X, which is train_df features)\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
        "numeric_features = X.select_dtypes(include=np.number).columns\n",
        "\n",
        "# Transformers\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough' # Keep other columns (like 'id' if not dropped)\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Hypertuning with RandomizedSearchCV\n",
        "# -----------------------------\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': [200, 400, 600, 800],\n",
        "    'classifier__max_depth': [3, 4, 5, 6, 8],\n",
        "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'classifier__subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    'classifier__colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
        "    'classifier__gamma': [0, 0.1, 0.2, 0.3],\n",
        "    'classifier__min_child_weight': [1, 3, 5],\n",
        "    'classifier__reg_lambda': [0.5, 1, 1.5],\n",
        "}\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    objective='multi:softmax',  # Changed to multi:softmax for multiclass\n",
        "    eval_metric='mlogloss',     # Changed to mlogloss for multiclass\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    tree_method='hist'\n",
        ")\n",
        "\n",
        "# Create a pipeline for hyperparameter tuning\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', xgb)\n",
        "])\n",
        "\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=pipeline, # Tuned on the pipeline\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20, # Reduced n_iter for quicker execution, can increase for better tuning\n",
        "    scoring='accuracy',\n",
        "    cv=cv_strategy,\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Starting Advanced XGBoost Hypertuning...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"\\n‚úÖ Hypertuning Complete!\")\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ Retrain final model on 100% train.csv with best parameters\n",
        "# -----------------------------\n",
        "print(\"\\nüèÅ Retraining Final Model on Full Data with Best Hyperparameters...\")\n",
        "\n",
        "# Extract the best parameters for the classifier step\n",
        "best_classifier_params = {\n",
        "    key.replace('classifier__', ''): value\n",
        "    for key, value in random_search.best_params_.items()\n",
        "    if key.startswith('classifier__')\n",
        "}\n",
        "\n",
        "# Define final XGBoost model with best parameters\n",
        "final_xgb_best = XGBClassifier(\n",
        "    **best_classifier_params, # Pass the extracted classifier params\n",
        "    objective='multi:softmax',\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    tree_method='hist'\n",
        ")\n",
        "\n",
        "# Create final pipeline with preprocessor and best model\n",
        "final_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', final_xgb_best)\n",
        "])\n",
        "\n",
        "# Train on the full training data\n",
        "final_pipeline.fit(X, y)\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ Predict on test.csv\n",
        "# -----------------------------\n",
        "print(\"\\nüßÆ Predicting on test.csv...\")\n",
        "# Prepare test data (excluding 'id' and 'WeightCategory' if it exists)\n",
        "X_test_submission = test_df.drop(columns=['id', 'WeightCategory'], errors='ignore')\n",
        "\n",
        "test_pred_encoded = final_pipeline.predict(X_test_submission)\n",
        "\n",
        "# Decode labels\n",
        "test_pred_labels = le.inverse_transform(test_pred_encoded)\n",
        "\n",
        "# -----------------------------\n",
        "# 9Ô∏è‚É£ Generate submission.csv\n",
        "# -----------------------------\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_df['id'],      # Use the 'id' column from the original test_df\n",
        "    \"WeightCategory\": test_pred_labels # Changed 'target' to 'WeightCategory'\n",
        "})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"\\n‚úÖ submission.csv generated successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3XRHJXB5ERT",
        "outputId": "d746eae8-1da0-4bb3-9d5c-746fbeeb1d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data Loaded Successfully!\n",
            "Train shape: (15533, 18) , Test shape: (5225, 17)\n",
            "\n",
            "üöÄ Starting Advanced XGBoost Hypertuning...\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "\n",
            "‚úÖ Hypertuning Complete!\n",
            "Best Hyperparameters: {'classifier__subsample': 1.0, 'classifier__reg_lambda': 0.5, 'classifier__n_estimators': 800, 'classifier__min_child_weight': 1, 'classifier__max_depth': 4, 'classifier__learning_rate': 0.1, 'classifier__gamma': 0.1, 'classifier__colsample_bytree': 0.8}\n",
            "\n",
            "üèÅ Retraining Final Model on Full Data with Best Hyperparameters...\n",
            "\n",
            "üßÆ Predicting on test.csv...\n",
            "\n",
            "‚úÖ submission.csv generated successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Advanced XGBoost Hypertuning + Final Model + Submission (No Early Stopping)\n",
        "# =============================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Load Data\n",
        "# -----------------------------\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "print(\"‚úÖ Data Loaded Successfully!\")\n",
        "print(\"Train shape:\", train_df.shape, \", Test shape:\", test_df.shape)\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Feature Engineering\n",
        "# -----------------------------\n",
        "for df in [train_df, test_df]:\n",
        "    df['BMI'] = df['Weight'] / (df['Height']**2)\n",
        "    df['AgeGroup'] = pd.cut(\n",
        "        df['Age'],\n",
        "        bins=[0, 18, 30, 45, 60, 100],\n",
        "        labels=['Teen', 'Young', 'Adult', 'MidAge', 'Senior']\n",
        "    )\n",
        "    if 'AgeGroup' in df.columns and df['AgeGroup'].isnull().sum() > 0:\n",
        "        mode_age_group = df['AgeGroup'].mode()[0]\n",
        "        df['AgeGroup'].fillna(mode_age_group, inplace=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Separate Features & Target\n",
        "# -----------------------------\n",
        "X = train_df.drop(columns=[\"WeightCategory\", \"id\"])\n",
        "y = train_df[\"WeightCategory\"]\n",
        "\n",
        "# Encode target\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Train-Validation Split\n",
        "# -----------------------------\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Preprocessing Pipeline\n",
        "# -----------------------------\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
        "numeric_features = X.select_dtypes(include=np.number).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', StandardScaler(), numeric_features),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "])\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Define XGBoost & Hyperparameter Ranges\n",
        "# -----------------------------\n",
        "xgb = XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    eval_metric='mlogloss',\n",
        "    use_label_encoder=False,\n",
        "    random_state=42,\n",
        "    tree_method='hist'\n",
        ")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', xgb)\n",
        "])\n",
        "\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': [400, 600, 800, 1000],\n",
        "    'classifier__max_depth': [4, 5, 6, 7],\n",
        "    'classifier__learning_rate': [0.01, 0.03, 0.05, 0.07],\n",
        "    'classifier__subsample': [0.75, 0.8, 0.85, 0.9],\n",
        "    'classifier__colsample_bytree': [0.75, 0.8, 0.85, 0.9],\n",
        "    'classifier__gamma': [0, 0.1, 0.2, 0.3],\n",
        "    'classifier__min_child_weight': [1, 2, 3, 4],\n",
        "    'classifier__reg_lambda': [1, 1.2, 1.5, 2.0]\n",
        "}\n",
        "\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,               # 50 combinations for better tuning\n",
        "    scoring='accuracy',\n",
        "    cv=cv_strategy,\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ Hyperparameter Search\n",
        "# -----------------------------\n",
        "print(\"\\nüöÄ Starting Advanced XGBoost Hypertuning...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"\\n‚úÖ Hypertuning Complete!\")\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ Retrain Final Model on Full Train Data\n",
        "# -----------------------------\n",
        "best_params = {key.replace('classifier__', ''): value\n",
        "               for key, value in random_search.best_params_.items()\n",
        "               if key.startswith('classifier__')}\n",
        "\n",
        "final_xgb = XGBClassifier(\n",
        "    **best_params,\n",
        "    objective='multi:softmax',\n",
        "    eval_metric='mlogloss',\n",
        "    use_label_encoder=False,\n",
        "    tree_method='hist',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "final_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', final_xgb)\n",
        "])\n",
        "\n",
        "print(\"\\nüèÅ Retraining Final Model on Full Training Data...\")\n",
        "final_pipeline.fit(X, y_encoded)\n",
        "\n",
        "# -----------------------------\n",
        "# 9Ô∏è‚É£ Evaluate Train & Validation Accuracy\n",
        "# -----------------------------\n",
        "y_train_pred = final_pipeline.predict(X)\n",
        "y_valid_pred = final_pipeline.predict(X_valid)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "print(f\"\\nüéØ Train Accuracy: {accuracy_score(y_encoded, y_train_pred):.4f}\")\n",
        "print(f\"üéØ Validation Accuracy: {accuracy_score(y_valid, y_valid_pred):.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# üîü Predict on Test Data & Generate Submission\n",
        "# -----------------------------\n",
        "X_test_submission = test_df.drop(columns=['id', 'WeightCategory'], errors='ignore')\n",
        "test_pred_encoded = final_pipeline.predict(X_test_submission)\n",
        "test_pred_labels = le.inverse_transform(test_pred_encoded)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_df['id'],\n",
        "    \"WeightCategory\": test_pred_labels\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"\\n‚úÖ submission.csv generated successfully!\")\n"
      ],
      "metadata": {
        "id": "GLkl8Dtnko9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the submission file\n",
        "files.download(\"submission.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "po350YAQ5Ozr",
        "outputId": "672a0abe-7513-469c-976e-7a9a0b51b28f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d318e643-ced6-4063-9788-484435338e7e\", \"submission.csv\", 120634)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}