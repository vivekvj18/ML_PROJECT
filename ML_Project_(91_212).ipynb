{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivekvj18/ML_PROJECT/blob/main/ML_Project_(91_212).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "7KDEs4OGknns",
        "outputId": "5434bdf2-709d-4709-b6d3-7e5b24a170e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ñ∂ Preprocessing (fit on train)...\n",
            "‚ñ∂ Fitting XGBoost (no early stopping)...\n",
            "\n",
            "=============================================\n",
            "MODEL PERFORMANCE (NO EARLY STOPPING)\n",
            "=============================================\n",
            "Training Accuracy: 0.9956\n",
            "Validation Accuracy: 0.9150\n",
            "Overfitting Gap: 8.06%\n",
            "=============================================\n",
            "\n",
            "‚ñ∂ Performing Stratified 5-Fold CV averaging (no early stopping)...\n",
            "  - Fold 1/5\n",
            "  - Fold 2/5\n",
            "  - Fold 3/5\n",
            "  - Fold 4/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2886261588.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mfold_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mxgb_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mfold_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# 2. CORRECTED: Get probability predictions from the fold model and accumulate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1681\u001b[0m             )\n\u001b[1;32m   1682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1684\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m             _check_call(\n\u001b[0;32m-> 2247\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2248\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ==================================================\n",
        "# XGBoost FINAL (NO EARLY STOPPING) + 5-FOLD CV AVERAGING\n",
        "# ==================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import files\n",
        "\n",
        "RND = 42\n",
        "\n",
        "# --- Load Data ---\n",
        "train_df = pd.read_csv(\"combined_data.csv\")\n",
        "test_df_original = pd.read_csv(\"test.csv\")  # keep original id column\n",
        "\n",
        "# =====================================================\n",
        "# --- FEATURE ENGINEERING (same as before) ---\n",
        "# =====================================================\n",
        "for df in [train_df, test_df_original]:\n",
        "    df['BMI'] = df['Weight'] / (df['Height']**2)\n",
        "    df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 18, 30, 45, 60, 100],\n",
        "                            labels=['Teen', 'Young', 'Adult', 'MidAge', 'Senior'])\n",
        "    df['BMICategory'] = pd.cut(df['BMI'], bins=[0, 18.5, 25, 30, 100],\n",
        "                               labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n",
        "    df['WeightHeightRatio'] = df['Weight'] / df['Height']\n",
        "    df['BMI_per_Age'] = df['BMI'] / df['Age'].replace(0, np.nan)\n",
        "    df['Height_per_Age'] = df['Height'] / df['Age'].replace(0, np.nan)\n",
        "    # --- Extra ratio-based features (new additions) ---\n",
        "    df['BMI_Age_Ratio'] = df['BMI'] / (df['Age'] + 1)\n",
        "    df['Age_Height_Ratio'] = df['Age'] / (df['Height'] + 1)\n",
        "\n",
        "\n",
        "\n",
        "    # Interaction features (kept for now)\n",
        "    df['Weight_x_Age'] = df['Weight'] * df['Age']\n",
        "    df['BMI_x_Height'] = df['BMI'] * df['Height']\n",
        "\n",
        "    if 'Gender' in df.columns:\n",
        "        df['Weight_z_byGender'] = df.groupby('Gender')['Weight'].transform(lambda x: (x - x.mean()) / x.std())\n",
        "        df['Height_z_byGender'] = df.groupby('Gender')['Height'].transform(lambda x: (x - x.mean()) / x.std())\n",
        "\n",
        "    if {'Gender', 'AgeGroup'} <= set(df.columns):\n",
        "        df['Gender_AgeGroup'] = df['Gender'].astype(str) + \"_\" + df['AgeGroup'].astype(str)\n",
        "\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df.fillna(df.median(numeric_only=True), inplace=True)\n",
        "\n",
        "# --- Prepare Training Data ---\n",
        "X = train_df.drop(columns=['WeightCategory', 'id'], errors='ignore')\n",
        "y = train_df['WeightCategory']\n",
        "\n",
        "if 'AgeGroup' in X.columns and X['AgeGroup'].isnull().sum() > 0:\n",
        "    X['AgeGroup'].fillna(X['AgeGroup'].mode()[0], inplace=True)\n",
        "\n",
        "numeric_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numeric_features),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "])\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# =====================================================\n",
        "# --- XGBoost PARAMETERS ---\n",
        "# =====================================================\n",
        "xgb_params = dict(\n",
        "    objective='multi:softprob',\n",
        "    num_class=len(le.classes_),\n",
        "    n_estimators=1200,        # allow longer learning\n",
        "    learning_rate=0.03,       # smaller step\n",
        "    max_depth=6,              # deeper trees for more complexity\n",
        "    min_child_weight=3,       # allows finer splits\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    gamma=0.15,               # slightly lower pruning\n",
        "    reg_alpha=0.1,            # L1 regularization\n",
        "    reg_lambda=1.5,           # L2 regularization\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=RND,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# STEP 1: Quick Train/Val evaluation (NO early stopping)\n",
        "# =====================================================\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=RND, stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(\"‚ñ∂ Preprocessing (fit on train)...\")\n",
        "preprocessor.fit(X_train)\n",
        "X_train_t = preprocessor.transform(X_train)\n",
        "X_val_t = preprocessor.transform(X_val)\n",
        "\n",
        "print(\"‚ñ∂ Fitting XGBoost (no early stopping)...\")\n",
        "model = XGBClassifier(**xgb_params)\n",
        "model.fit(X_train_t, y_train)  # no early stopping\n",
        "\n",
        "train_acc = accuracy_score(y_train, model.predict(X_train_t))\n",
        "val_acc = accuracy_score(y_val, model.predict(X_val_t))\n",
        "print(\"\\n=============================================\")\n",
        "print(\"MODEL PERFORMANCE (NO EARLY STOPPING)\")\n",
        "print(\"=============================================\")\n",
        "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Overfitting Gap: {(train_acc - val_acc)*100:.2f}%\")\n",
        "print(\"=============================================\\n\")\n",
        "\n",
        "# =====================================================\n",
        "# STEP 2: Stratified 5-Fold CV Averaging (CORRECTED)\n",
        "# =====================================================\n",
        "print(\"‚ñ∂ Performing Stratified 5-Fold CV averaging (no early stopping)...\")\n",
        "skf = StratifiedKFold(n_splits=8, shuffle=True, random_state=RND)\n",
        "test_data = test_df_original.drop(columns=['id', 'WeightCategory'], errors='ignore')\n",
        "\n",
        "# 1. Initialize array to accumulate PROBABILITY scores, not just predictions.\n",
        "test_preds_proba = np.zeros((test_data.shape[0], len(le.classes_)))\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_encoded), 1):\n",
        "    print(f\"  - Fold {fold}/5\")\n",
        "    X_tr = X.iloc[train_idx]\n",
        "    y_tr = y_encoded[train_idx]\n",
        "\n",
        "    # fit preprocessor on training fold\n",
        "    preprocessor.fit(X_tr)\n",
        "    X_tr_t = preprocessor.transform(X_tr)\n",
        "    X_test_t = preprocessor.transform(test_data)\n",
        "\n",
        "    fold_model = XGBClassifier(**xgb_params)\n",
        "    fold_model.fit(X_tr_t, y_tr)\n",
        "\n",
        "    # 2. CORRECTED: Get probability predictions from the fold model and accumulate\n",
        "    test_preds_proba += fold_model.predict_proba(X_test_t)\n",
        "\n",
        "\n",
        "# 3. Average the accumulated probabilities and convert to final labels\n",
        "test_preds_avg = test_preds_proba / skf.n_splits\n",
        "test_pred_encoded = np.argmax(test_preds_avg, axis=1)\n",
        "test_pred_labels = le.inverse_transform(test_pred_encoded)\n",
        "print(\"‚ñ∂ CV-averaged test predictions ready.\")\n",
        "\n",
        "# =====================================================\n",
        "# STEP 3: (Optional) Retrain on FULL training data for final model\n",
        "# =====================================================\n",
        "print(\"‚ñ∂ Retraining model on full training data (no early stopping)...\")\n",
        "preprocessor.fit(X)\n",
        "X_full_t = preprocessor.transform(X)\n",
        "final_model = XGBClassifier(**xgb_params)\n",
        "final_model.fit(X_full_t, y_encoded)  # no early stopping\n",
        "print(\"‚ñ∂ Retrain complete.\")\n",
        "\n",
        "# =====================================================\n",
        "# STEP 4: Save submission (using CV-averaged predictions)\n",
        "# =====================================================\n",
        "#submission_file = 'kaggle_submission_xgb_no_earlystop_cvavg_FIXED.csv'\n",
        "#submission_df = pd.DataFrame({\n",
        "#    'id': test_df_original['id'],\n",
        "#    'WeightCategory': test_pred_labels # Ensure this is the corrected variable\n",
        "#})\n",
        "#submission_df.to_csv(submission_file, index=False)\n",
        "#print(f\"‚úÖ Submission file created: {submission_file}\")\n",
        "\n",
        "# Uncomment to auto-download in Colab:\n",
        "# files.download(submission_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_file = 'kaggle_submission_xgb_no_earlystop_cvavg_FIXED.csv'\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': test_df_original['id'],\n",
        "    'WeightCategory': test_pred_labels # Ensure this is the corrected variable\n",
        "})\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "print(f\"‚úÖ Submission file created: {submission_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7prbgS8kpaV",
        "outputId": "9bbe8e3e-3384-42b0-86d7-2730bd89c1b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Submission file created: kaggle_submission_xgb_no_earlystop_cvavg_FIXED.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üèÜ XGBoost FINAL (NO EARLY STOPPING) + 5-FOLD CV AVERAGING\n",
        "# Optimized for Kaggle (Single Model Only)\n",
        "# ==================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import files\n",
        "\n",
        "RND = 42\n",
        "np.random.seed(RND)\n",
        "\n",
        "# ==================================================\n",
        "# --- LOAD DATA ---\n",
        "# ==================================================\n",
        "train_df = pd.read_csv(\"combined_data.csv\")\n",
        "test_df_original = pd.read_csv(\"test.csv\")  # keep id column for submission\n",
        "\n",
        "# ==================================================\n",
        "# --- FEATURE ENGINEERING ---\n",
        "# ==================================================\n",
        "for df in [train_df, test_df_original]:\n",
        "    # --- Base ratios ---\n",
        "    df['BMI'] = df['Weight'] / (df['Height'] ** 2)\n",
        "    df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 18, 30, 45, 60, 100],\n",
        "                            labels=['Teen', 'Young', 'Adult', 'MidAge', 'Senior'])\n",
        "    df['BMICategory'] = pd.cut(df['BMI'], bins=[0, 18.5, 25, 30, 100],\n",
        "                               labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n",
        "    df['WeightHeightRatio'] = df['Weight'] / df['Height']\n",
        "    df['BMI_per_Age'] = df['BMI'] / df['Age'].replace(0, np.nan)\n",
        "    df['Height_per_Age'] = df['Height'] / df['Age'].replace(0, np.nan)\n",
        "    df['BMI_Age_Ratio'] = df['BMI'] / (df['Age'] + 1)\n",
        "    df['Age_Height_Ratio'] = df['Age'] / (df['Height'] + 1)\n",
        "    df['Weight_x_Age'] = df['Weight'] * df['Age']\n",
        "    df['BMI_x_Height'] = df['BMI'] * df['Height']\n",
        "\n",
        "    # --- Gender normalization ---\n",
        "    if 'Gender' in df.columns:\n",
        "        df['Weight_z_byGender'] = df.groupby('Gender')['Weight'].transform(lambda x: (x - x.mean()) / x.std())\n",
        "        df['Height_z_byGender'] = df.groupby('Gender')['Height'].transform(lambda x: (x - x.mean()) / x.std())\n",
        "\n",
        "    # --- Categorical interaction ---\n",
        "    if {'Gender', 'AgeGroup'} <= set(df.columns):\n",
        "        df['Gender_AgeGroup'] = df['Gender'].astype(str) + \"_\" + df['AgeGroup'].astype(str)\n",
        "\n",
        "    # --- NEW extra engineered ratios & products ---\n",
        "    if all(col in df.columns for col in ['HCVC', 'NCP', 'CH2O', 'FAF', 'TUE', 'Water']):\n",
        "        df['HCVC_NCP_Ratio'] = df['HCVC'] / (df['NCP'] + 1)\n",
        "        df['CH2O_FAF_Product'] = df['CH2O'] * df['FAF']\n",
        "        df['FAF_TUE_Ratio'] = df['FAF'] / (df['TUE'] + 1)\n",
        "        df['Water_CH2O_Ratio'] = df['Water'] / (df['CH2O'] + 1)\n",
        "        df['BMIxFAF'] = df['BMI'] * df['FAF']\n",
        "\n",
        "    # --- Handle inf/nan ---\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df.fillna(df.median(numeric_only=True), inplace=True)\n",
        "\n",
        "# ==================================================\n",
        "# --- PREPARE TRAINING DATA ---\n",
        "# ==================================================\n",
        "X = train_df.drop(columns=['WeightCategory', 'id'], errors='ignore')\n",
        "y = train_df['WeightCategory']\n",
        "\n",
        "if 'AgeGroup' in X.columns and X['AgeGroup'].isnull().sum() > 0:\n",
        "    X['AgeGroup'].fillna(X['AgeGroup'].mode()[0], inplace=True)\n",
        "\n",
        "numeric_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numeric_features),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "])\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# ==================================================\n",
        "# --- XGBOOST PARAMETERS (TUNED) ---\n",
        "# ==================================================\n",
        "xgb_params = dict(\n",
        "    objective='multi:softprob',\n",
        "    num_class=len(le.classes_),\n",
        "    n_estimators=1200,          # Must be higher for smaller learning rate\n",
        "    learning_rate=0.02,         # Much slower learning\n",
        "    max_depth=4,                # Conservative tree depth\n",
        "    subsample=0.75,\n",
        "    colsample_bytree=0.75,\n",
        "    gamma=0.5,                  # Stricter pruning\n",
        "    min_child_weight=6,\n",
        "    reg_alpha=0.01,           # L2 regularization\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=RND,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "\n",
        "# ==================================================\n",
        "# --- TRAIN / VALIDATION SPLIT (Quick Check) ---\n",
        "# ==================================================\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=RND, stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(\"‚ñ∂ Preprocessing (fit on train)...\")\n",
        "preprocessor.fit(X_train)\n",
        "X_train_t = preprocessor.transform(X_train)\n",
        "X_val_t = preprocessor.transform(X_val)\n",
        "\n",
        "print(\"‚ñ∂ Training single XGBoost model (no early stopping)...\")\n",
        "model = XGBClassifier(**xgb_params)\n",
        "model.fit(X_train_t, y_train)\n",
        "\n",
        "train_acc = accuracy_score(y_train, model.predict(X_train_t))\n",
        "val_acc = accuracy_score(y_val, model.predict(X_val_t))\n",
        "print(\"\\n=============================================\")\n",
        "print(\"MODEL PERFORMANCE (NO EARLY STOPPING)\")\n",
        "print(\"=============================================\")\n",
        "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Overfitting Gap: {(train_acc - val_acc)*100:.2f}%\")\n",
        "print(\"=============================================\\n\")\n",
        "\n",
        "# ==================================================\n",
        "# --- STRATIFIED 5-FOLD CV AVERAGING ---\n",
        "# ==================================================\n",
        "print(\"‚ñ∂ Performing Stratified 5-Fold CV averaging (no early stopping)...\")\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RND)\n",
        "test_data = test_df_original.drop(columns=['id', 'WeightCategory'], errors='ignore')\n",
        "\n",
        "test_preds_proba = np.zeros((test_data.shape[0], len(le.classes_)))\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_encoded), 1):\n",
        "    print(f\"  - Fold {fold}/5\")\n",
        "    X_tr = X.iloc[train_idx]\n",
        "    y_tr = y_encoded[train_idx]\n",
        "\n",
        "    preprocessor.fit(X_tr)\n",
        "    X_tr_t = preprocessor.transform(X_tr)\n",
        "    X_test_t = preprocessor.transform(test_data)\n",
        "\n",
        "    fold_model = XGBClassifier(**xgb_params)\n",
        "    fold_model.fit(X_tr_t, y_tr)\n",
        "\n",
        "    test_preds_proba += fold_model.predict_proba(X_test_t)\n",
        "\n",
        "# --- Average predicted probabilities ---\n",
        "test_preds_avg = test_preds_proba / skf.n_splits\n",
        "test_pred_encoded = np.argmax(test_preds_avg, axis=1)\n",
        "test_pred_labels = le.inverse_transform(test_pred_encoded)\n",
        "print(\"‚ñ∂ CV-averaged test predictions ready.\")\n",
        "\n",
        "# ==================================================\n",
        "# --- RETRAIN ON FULL TRAIN DATA (FINAL MODEL) ---\n",
        "# ==================================================\n",
        "print(\"‚ñ∂ Retraining model on full training data (no early stopping)...\")\n",
        "preprocessor.fit(X)\n",
        "X_full_t = preprocessor.transform(X)\n",
        "final_model = XGBClassifier(**xgb_params)\n",
        "final_model.fit(X_full_t, y_encoded)\n",
        "print(\"‚ñ∂ Retrain complete.\")\n",
        "\n",
        "# ==================================================\n",
        "# --- SAVE SUBMISSION ---\n",
        "# ==================================================\n",
        "#submission_file = 'kaggle_submission_xgb_FINAL.csv'\n",
        "#submission_df = pd.DataFrame({\n",
        "#    'id': test_df_original['id'],\n",
        "#    'WeightCategory': test_pred_labels\n",
        "#})\n",
        "#submission_df.to_csv(submission_file, index=False)\n",
        "#print(f\"‚úÖ Submission file created: {submission_file}\")\n",
        "\n",
        "# Uncomment in Colab to auto-download:\n",
        "# files.download(submission_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXzJ5axQ80Qe",
        "outputId": "4548a34b-ee6a-4928-ff0d-9036a9f144f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ñ∂ Preprocessing (fit on train)...\n",
            "‚ñ∂ Training single XGBoost model (no early stopping)...\n",
            "\n",
            "=============================================\n",
            "MODEL PERFORMANCE (NO EARLY STOPPING)\n",
            "=============================================\n",
            "Training Accuracy: 0.9508\n",
            "Validation Accuracy: 0.9158\n",
            "Overfitting Gap: 3.50%\n",
            "=============================================\n",
            "\n",
            "‚ñ∂ Performing Stratified 5-Fold CV averaging (no early stopping)...\n",
            "  - Fold 1/5\n",
            "  - Fold 2/5\n",
            "  - Fold 3/5\n",
            "  - Fold 4/5\n",
            "  - Fold 5/5\n",
            "‚ñ∂ CV-averaged test predictions ready.\n",
            "‚ñ∂ Retraining model on full training data (no early stopping)...\n",
            "‚ñ∂ Retrain complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_file = 'kaggle_submission_xgb_FINAL.csv'\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': test_df_original['id'],\n",
        "    'WeightCategory': test_pred_labels\n",
        "})\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "print(f\"‚úÖ Submission file created: {submission_file}\")\n",
        "\n",
        "# Uncomment in Colab to auto-download:\n",
        "# files.download(submission_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUaM1goK-1Xa",
        "outputId": "0fce966f-eeaf-433f-ccea-8997edaf0d3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Submission file created: kaggle_submission_xgb_FINAL.csv\n"
          ]
        }
      ]
    }
  ]
}